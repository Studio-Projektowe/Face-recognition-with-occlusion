{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dd9e547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\User\\OneDrive\\Dokumenty\\Face-Recognition\\venv3.11\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Wczytywanie assetu okularów z: okulary/oksy.png...\n",
      "Asset okularów wczytany pomyślnie.\n",
      "Ładowanie modelu RetinaFace do pamięci GPU...\n",
      "Detektor RetinaFace gotowy.\n"
     ]
    }
   ],
   "source": [
    "# Importowanie bibliotek\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "from retinaface import RetinaFace # Lub import, którego używa wasza biblioteka\n",
    "import os\n",
    "import io\n",
    "import math\n",
    "from tqdm.auto import tqdm # Do paska postępu\n",
    "import tensorflow as tf # Do czytania i pisania w GCS\n",
    "\n",
    "# --- KONFIGURACJA (Zmieńcie te wartości) ---\n",
    "\n",
    "# 1. Ścieżki do \"Magazynów\" (waszych bucketów GCS)\n",
    "# BUCKET_ORYGINALNY = \"kaggle-casia-dataset-original-...\" # Wiadro z danymi z Kaggle\n",
    "# BUCKET_OKLUZJI = \"kaggle-casia-dataset-occluded-...\" # NOWE, PUSTE wiadro na wyniki\n",
    "\n",
    "# # 2. Nazwa folderu wejściowego (tam, gdzie `script.sh` wrzucił pliki)\n",
    "# FOLDER_WEJSCIOWY = \"CASIA-WebFace\" # Sprawdźcie w GCS, czy ta nazwa się zgadza!\n",
    "\n",
    "# # 3. Nazwa folderu wyjściowego (jak mają się nazywać nowe dane)\n",
    "# FOLDER_WYJSCIOWY = \"casia-webface-occluded-sunglasses\"\n",
    "\n",
    "# 4. Nazwa pliku z okularami (musi być w GCS)\n",
    "# OKULARY_PNG_PATH = f\"gs://{BUCKET_ORYGINALNY}/assets/sunglasses.png\"\n",
    "OKULARY_PNG_PATH = \"okulary/oksy.png\"\n",
    "\n",
    "# --- Koniec Konfiguracji ---\n",
    "\n",
    "\n",
    "# Inicjalizacja \"Maszyn\" (globalne instancje)\n",
    "\n",
    "# 1. Wczytaj asset okularów z GCS RAZ (oszczędza czas)\n",
    "try:\n",
    "    print(f\"Wczytywanie assetu okularów z: {OKULARY_PNG_PATH}...\")\n",
    "    with tf.io.gfile.GFile(OKULARY_PNG_PATH, 'rb') as f:\n",
    "        sunglasses_asset_bytes = f.read()\n",
    "    \n",
    "    # Konwertujemy na obraz PIL z kanałem alfa (RGBA)\n",
    "    sunglasses_img = Image.open(io.BytesIO(sunglasses_asset_bytes)).convert(\"RGBA\")\n",
    "    print(\"Asset okularów wczytany pomyślnie.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"BŁĄD KRYTYCZNY: Nie udało się wczytać pliku {OKULARY_PNG_PATH}\")\n",
    "    print(\"Upewnij się, że wrzuciłaś plik `sunglasses.png` do bucketa GCS!\")\n",
    "    # Tutaj można przerwać działanie notatnika\n",
    "\n",
    "\n",
    "# 2. Uruchom detektor RetinaFace (niech od razu załaduje model do pamięci GPU)\n",
    "# To jest KRYTYCZNE dla wydajności - robimy to raz, a nie 490,000 razy w pętli!\n",
    "try:\n",
    "    print(\"Ładowanie modelu RetinaFace do pamięci GPU...\")\n",
    "    # (Poniższa linia to tylko przykład, użyjcie waszej implementacji)\n",
    "    # Upewnijcie się, że RetinaFace używa GPU (`gpu_id=0`)\n",
    "    print(\"Detektor RetinaFace gotowy.\")\n",
    "except Exception as e:\n",
    "    print(f\"BŁĄD KRYTYCZNY: Nie udało się załadować RetinaFace: {e}\")\n",
    "    print(\"Sprawdź, czy biblioteka jest poprawnie zainstalowana i czy TensorFlow widzi GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bd184e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_detect_face(image_bytes):\n",
    "    \"\"\"\n",
    "    Wczytuje obraz z bajtów i zwraca kluczowe punkty twarzy.\n",
    "    \n",
    "    Zwraca:\n",
    "    - None: Jeśli wystąpił błąd lub nie znaleziono twarzy.\n",
    "    - dict: Słownik zawierający 'left_eye', 'right_eye' i 'nose_tip'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Konwertujemy bajty na obraz PIL i potem na NumPy (format dla detektora)\n",
    "        image_pil = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
    "        image_np = np.array(image_pil)\n",
    "    except Exception as e:\n",
    "        print(f\"[BŁĄD WCZYTANIA]: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Używamy detektora (który jest już w GPU) do znalezienia twarzy\n",
    "    try:\n",
    "        # Użyj waszej implementacji RetinaFace.\n",
    "        # To jest tylko przykład, jak to może wyglądać:\n",
    "        results = RetinaFace.detect_faces(image_np) \n",
    "        # Obsługa błędu: co jeśli twarz nie zostanie wykryta?\n",
    "        if not results or len(results) == 0:\n",
    "            return None # Zwracamy None, żeby główna pętla pominęła ten plik\n",
    "\n",
    "        # Bierzemy tylko pierwszą, najbardziej prawdopodobną twarz\n",
    "        face_data = results['face_1']\n",
    "        keypoints = face_data['landmarks']\n",
    "        \n",
    "        # Zwracamy tylko te punkty, których potrzebujemy\n",
    "        return {\n",
    "            'left_eye': keypoints['left_eye'],   # (x, y)\n",
    "            'right_eye': keypoints['right_eye'], # (x, y)\n",
    "            'nose_tip': keypoints['nose'],       # (x, y)\n",
    "            'image_pil': image_pil # Zwracamy też wczytany obraz, żeby nie robić tego dwa razy\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[BŁĄD DETEKCJI]: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97a470df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_transformations(face_landmarks):\n",
    "    \"\"\"\n",
    "    Oblicza skalę, kąt obrotu i pozycję dla okularów\n",
    "    na podstawie punktów kluczowych twarzy.\n",
    "    \"\"\"\n",
    "    # 1. Pobierz współrzędne\n",
    "    left_eye = np.array(face_landmarks['left_eye'])\n",
    "    right_eye = np.array(face_landmarks['right_eye'])\n",
    "    \n",
    "    # 2. Oblicz KĄT OBROTU\n",
    "    # Obliczamy różnicę w osi Y i X między oczami\n",
    "    dX = right_eye[1] - left_eye[1]\n",
    "    dY = right_eye[0] - left_eye[0]\n",
    "    \n",
    "    # Trygonometria (atan2) da nam kąt w radianach\n",
    "    angle_rad = math.atan2(dY, dX)\n",
    "    # Konwertujemy na stopnie dla biblioteki PIL\n",
    "    angle_deg = math.degrees(angle_rad)\n",
    "    \n",
    "    # 3. Oblicz SKALĘ (Wielkość okularów)\n",
    "    # Chcemy, żeby okulary były 1.5x szersze niż odległość między oczami\n",
    "    # (Możecie dostosować ten mnożnik `1.5`!)\n",
    "    SCALE_FACTOR = 1.5 \n",
    "    \n",
    "    # Odległość Euklidesowa między oczami\n",
    "    eye_distance = np.linalg.norm(right_eye - left_eye)\n",
    "    \n",
    "    # Nasz obrazek `sunglasses.png` ma jakąś oryginalną szerokość\n",
    "    original_sunglasses_width = sunglasses_img.width \n",
    "    \n",
    "    # Obliczamy nową szerokość i skalę\n",
    "    target_sunglasses_width = eye_distance * SCALE_FACTOR\n",
    "    scale = target_sunglasses_width / original_sunglasses_width\n",
    "    \n",
    "    # 4. Oblicz POZYCJĘ (Środek okularów)\n",
    "    # Chcemy, żeby środek okularów wylądował dokładnie\n",
    "    # na środku linii łączącej oczy.\n",
    "    eye_center = (left_eye + right_eye) / 2\n",
    "    \n",
    "    return {\n",
    "        'angle': angle_deg,\n",
    "        'scale': scale,\n",
    "        'position': (int(eye_center[0]), int(eye_center[1]))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80de0824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_occlusion(original_face_image, sunglasses_asset, transform):\n",
    "    \"\"\"\n",
    "    Nakłada przygotowany asset okularów na obraz twarzy\n",
    "    używając obliczonych transformacji.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Pobierz transformacje\n",
    "        scale = transform['scale']\n",
    "        angle = transform['angle']\n",
    "        position_xy = transform['position'] # (x, y) środek\n",
    "\n",
    "        # 2. Stwórz kopię okularów, żeby nie psuć oryginału\n",
    "        sunglasses_copy = sunglasses_asset.copy()\n",
    "\n",
    "        # 3. SKALOWANIE\n",
    "        # Oblicz nowy rozmiar na podstawie skali\n",
    "        new_width = int(sunglasses_copy.width * scale)\n",
    "        new_height = int(sunglasses_copy.height * scale)\n",
    "        \n",
    "        # Użyj `Resampling.LANCZOS` dla najlepszej jakości\n",
    "        sunglasses_resized = sunglasses_copy.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "        \n",
    "        # 4. OBRÓT\n",
    "        # Obracamy wokół środka. `expand=True` zapewnia, że nic nie zostanie obcięte.\n",
    "        sunglasses_rotated = sunglasses_resized.rotate(angle, expand=True, resample=Image.Resampling.BICUBIC)\n",
    "\n",
    "        # 5. POZYCJONOWANIE (Obliczanie lewego górnego rogu)\n",
    "        # PIL wkleja obrazy używając lewego górnego rogu (x_pos, y_pos).\n",
    "        # My mamy środek (`position_xy`). Musimy to przeliczyć.\n",
    "        \n",
    "        center_x = position_xy[0]\n",
    "        center_y = position_xy[1]\n",
    "        \n",
    "        # Obliczamy pozycję lewego górnego rogu\n",
    "        x_pos = center_x - (sunglasses_rotated.width // 2)\n",
    "        y_pos = center_y - (sunglasses_rotated.height // 2)\n",
    "        \n",
    "        # 6. NAKŁADANIE (Magia kanału Alfa)\n",
    "        # Stwórz kopię oryginalnego zdjęcia, na której będziemy malować\n",
    "        output_image = original_face_image.copy()\n",
    "        \n",
    "        # `paste` z kanałem alfa (RGBA) jako maską robi dokładnie to, co chcemy\n",
    "        # - wkleja tylko okulary, zostawiając tło przezroczyste.\n",
    "        output_image.paste(sunglasses_rotated, (x_pos, y_pos), mask=sunglasses_rotated)\n",
    "        \n",
    "        return output_image # Zwracamy gotowy obrazek\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[BŁĄD NAKŁADANIA]: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1892083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_image(input_file_path, base_input_path, base_output_path):\n",
    "    \"\"\"\n",
    "    Pełny potok dla jednego obrazu: Wczytaj, Wykryj, Oblicz, Nałóż, Zapisz.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Wczytaj surowe bajty obrazu z GCS\n",
    "        with tf.io.gfile.GFile(input_file_path, 'rb') as f:\n",
    "            image_bytes = f.read()\n",
    "        \n",
    "        # 2. Uruchom Krok 2: Detekcja\n",
    "        detection_data = load_and_detect_face(image_bytes)\n",
    "        \n",
    "        # 3. Obsługa błędu (jeśli nie znaleziono twarzy)\n",
    "        if detection_data is None:\n",
    "            return \"failed_detection\" # Zwracamy powód błędu\n",
    "            \n",
    "        original_image = detection_data['image_pil']\n",
    "            \n",
    "        # 4. Uruchom Krok 3: Geometria\n",
    "        transform_data = calculate_transformations(detection_data)\n",
    "        \n",
    "        # 5. Uruchom Krok 4: Nakładanie\n",
    "        # (sunglasses_img to nasza globalna zmienna z Kroku 1)\n",
    "        final_image_pil = apply_occlusion(original_image, sunglasses_img, transform_data)\n",
    "\n",
    "        if final_image_pil is None:\n",
    "            return \"failed_overlay\"\n",
    "            \n",
    "        # 6. Zapisz wynik z powrotem do GCS\n",
    "        \n",
    "        # Obliczamy ścieżkę zapisu, zachowując strukturę folderów\n",
    "        relative_path = os.path.relpath(input_file_path, start=base_input_path)\n",
    "        output_file_path = f\"{base_output_path}/{relative_path}\"\n",
    "        print(\"Where: \", output_file_path)\n",
    "        \n",
    "        # Upewnij się, że folder docelowy istnieje w GCS\n",
    "        output_dir = os.path.dirname(output_file_path)\n",
    "        if not tf.io.gfile.exists(output_dir):\n",
    "            tf.io.gfile.makedirs(output_dir)\n",
    "            \n",
    "        # Zapisujemy obrazek w pamięci (jako JPEG) i wysyłamy do GCS\n",
    "        with io.BytesIO() as output_buffer:\n",
    "            final_image_pil.save(output_buffer, format=\"JPEG\")\n",
    "            image_data_to_write = output_buffer.getvalue()\n",
    "            \n",
    "            with tf.io.gfile.GFile(output_file_path, 'wb') as f_out:\n",
    "                f_out.write(image_data_to_write)\n",
    "                \n",
    "        return \"success\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[BŁĄD KRYTYCZNY] przy pliku {input_file_path}: {e}\")\n",
    "        return \"failed_critical\"\n",
    "\n",
    "# --- GŁÓWNA PĘTLA URUCHOMIENIOWA ---\n",
    "def run_augmentation_pipeline():\n",
    "    print(\"Uruchamianie pełnej linii produkcyjnej augmentacji...\")\n",
    "    \n",
    "    # Używamy ścieżek zdefiniowanych w Kroku 1 (`augmentacja_0_setup.md`)\n",
    "    # base_input_path = f\"gs://{BUCKET_ORYGINALNY}/{FOLDER_WEJSCIOWY}\"\n",
    "    # base_output_path = f\"gs://{BUCKET_OKLUZJI}/{FOLDER_WYJSCIOWY}\"\n",
    "\n",
    "    base_input_path = \"images\"\n",
    "    base_output_path = \"occluded\"\n",
    "    \n",
    "    print(f\"Szukanie plików w: {base_input_path}...\")\n",
    "    \n",
    "    # Znajdź wszystkie pliki .jpg w folderze i podfolderach (czemu jpg??)\n",
    "    # all_image_paths = tf.io.gfile.glob(f\"{base_input_path}/*.png\")\n",
    "    all_image_paths = [\"./images/ewa.png\", './images/3_faces.png', \"./images/face_with_landmarks.jpg\"]\n",
    "\n",
    "    # Uruchomienie pętli z paskiem postępu tqdm\n",
    "    # (Można to zrównoleglić, ale na razie zróbmy to sekwencyjnie)\n",
    "    \n",
    "    stats = {\"success\": 0, \"failed_detection\": 0, \"failed_overlay\": 0, \"failed_critical\": 0}\n",
    "    \n",
    "    # Możecie zmniejszyć liczbę plików do testów, np. biorąc tylko 1000 pierwszych:\n",
    "    # all_image_paths = all_image_paths[:1000] \n",
    "    \n",
    "    print(f\"Znaleziono {len(all_image_paths)} obrazów. Rozpoczynam przetwarzanie...\")\n",
    "    \n",
    "    for path in tqdm(all_image_paths, desc=\"Przetwarzanie obrazów\"):\n",
    "        result_status = process_single_image(path, base_input_path, base_output_path)\n",
    "        stats[result_status] += 1\n",
    "        \n",
    "    print(\"\\n--- ZAKOŃCZONO ---\")\n",
    "    print(f\"Pomyślnie przetworzono: {stats['success']}\")\n",
    "    print(f\"Pominięto (nie znaleziono twarzy): {stats['failed_detection']}\")\n",
    "    print(f\"Pominięto (błąd nakładania): {stats['failed_overlay']}\")\n",
    "    print(f\"Pominięto (błąd krytyczny): {stats['failed_critical']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc02ac4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uruchamianie pełnej linii produkcyjnej augmentacji...\n",
      "Szukanie plików w: images...\n",
      "Znaleziono 3 obrazów. Rozpoczynam przetwarzanie...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Przetwarzanie obrazów:  33%|███▎      | 1/3 [00:11<00:22, 11.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where:  occluded/ewa.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Przetwarzanie obrazów:  67%|██████▋   | 2/3 [00:19<00:09,  9.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where:  occluded/3_faces.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Przetwarzanie obrazów: 100%|██████████| 3/3 [00:28<00:00,  9.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where:  occluded/face_with_landmarks.jpg\n",
      "\n",
      "--- ZAKOŃCZONO ---\n",
      "Pomyślnie przetworzono: 3\n",
      "Pominięto (nie znaleziono twarzy): 0\n",
      "Pominięto (błąd nakładania): 0\n",
      "Pominięto (błąd krytyczny): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Aby to wszystko uruchomić, po prostu odpal tę funkcję w komórce notatnika:\n",
    "run_augmentation_pipeline()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
